{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T05:58:22.642004Z",
     "iopub.status.busy": "2025-03-29T05:58:22.641696Z",
     "iopub.status.idle": "2025-03-29T05:58:30.220569Z",
     "shell.execute_reply": "2025-03-29T05:58:30.219928Z",
     "shell.execute_reply.started": "2025-03-29T05:58:22.641979Z"
    },
    "executionInfo": {
     "elapsed": 10460,
     "status": "ok",
     "timestamp": 1742290020456,
     "user": {
      "displayName": "Soham Kalburgi",
      "userId": "01384065308802855965"
     },
     "user_tz": -330
    },
    "id": "tv9LTP9MrhKx",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import wandb\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\" \n",
    "https://github.com/HanxunH/Active-Passive-Losses/\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T05:58:30.221716Z",
     "iopub.status.busy": "2025-03-29T05:58:30.221523Z",
     "iopub.status.idle": "2025-03-29T05:58:30.271509Z",
     "shell.execute_reply": "2025-03-29T05:58:30.270507Z",
     "shell.execute_reply.started": "2025-03-29T05:58:30.221699Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1742290020461,
     "user": {
      "displayName": "Soham Kalburgi",
      "userId": "01384065308802855965"
     },
     "user_tz": -330
    },
    "id": "lpXqP_Z7quAV",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwlPgj4PX_p7"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T05:58:30.273625Z",
     "iopub.status.busy": "2025-03-29T05:58:30.273381Z",
     "iopub.status.idle": "2025-03-29T05:58:30.293067Z",
     "shell.execute_reply": "2025-03-29T05:58:30.292246Z",
     "shell.execute_reply.started": "2025-03-29T05:58:30.273603Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1742290020464,
     "user": {
      "displayName": "Soham Kalburgi",
      "userId": "01384065308802855965"
     },
     "user_tz": -330
    },
    "id": "tuITkIbVjp1v",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def other_class(n_classes, current_class):\n",
    "    \"\"\"\n",
    "    Returns a list of class indices excluding the class indexed by class_ind\n",
    "    :param nb_classes: number of classes in the task\n",
    "    :param class_ind: the class index to be omitted\n",
    "    :return: one random class that != class_ind\n",
    "    \"\"\"\n",
    "    if current_class < 0 or current_class >= n_classes:\n",
    "        error_str = \"class_ind must be within the range (0, nb_classes - 1)\"\n",
    "        raise ValueError(error_str)\n",
    "\n",
    "    other_class_list = list(range(n_classes))\n",
    "    other_class_list.remove(current_class)\n",
    "    other_class = np.random.choice(other_class_list)\n",
    "    return other_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T05:58:30.294699Z",
     "iopub.status.busy": "2025-03-29T05:58:30.294409Z",
     "iopub.status.idle": "2025-03-29T05:58:30.310017Z",
     "shell.execute_reply": "2025-03-29T05:58:30.309368Z",
     "shell.execute_reply.started": "2025-03-29T05:58:30.294670Z"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1742290020476,
     "user": {
      "displayName": "Soham Kalburgi",
      "userId": "01384065308802855965"
     },
     "user_tz": -330
    },
    "id": "Paa-z_MnQ50x",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Cifar10Noisy(datasets.CIFAR10):\n",
    "   def __init__(self, root, train=True, transform=None, target_transform=None, download=True, noise_rate=0.0, asym=False, seed=0):\n",
    "    super(Cifar10Noisy, self).__init__(root, download=download, transform=transform, target_transform=target_transform)\n",
    "    if asym:\n",
    "      # automobile < - truck, bird -> airplane, cat <-> dog, deer -> horse\n",
    "      source_class = [9, 2, 3, 5, 4]\n",
    "      target_class = [1, 0, 5, 3, 7]\n",
    "      for s, t in zip(source_class, target_class):\n",
    "          cls_idx = np.where(np.array(self.targets) == s)[0]\n",
    "          n_noisy = int(noise_rate * cls_idx.shape[0])\n",
    "          noisy_sample_index = np.random.choice(cls_idx, n_noisy, replace=False)\n",
    "          for idx in noisy_sample_index:\n",
    "              self.targets[idx] = t\n",
    "      return\n",
    "    elif noise_rate > 0:\n",
    "      n_samples = len(self.targets)\n",
    "      n_noisy = int(noise_rate * n_samples)\n",
    "      print(\"%d Noisy samples\" % (n_noisy))\n",
    "      class_index = [np.where(np.array(self.targets) == i)[0] for i in range(10)]\n",
    "      class_noisy = int(n_noisy / 10)\n",
    "      noisy_idx = []\n",
    "      for d in range(10):\n",
    "          noisy_class_index = np.random.choice(class_index[d], class_noisy, replace=False)\n",
    "          noisy_idx.extend(noisy_class_index)\n",
    "          print(\"Class %d, number of noisy % d\" % (d, len(noisy_class_index)))\n",
    "      for i in noisy_idx:\n",
    "          self.targets[i] = other_class(n_classes=10, current_class=self.targets[i])\n",
    "      print(len(noisy_idx))\n",
    "      print(\"Print noisy label generation statistics:\")\n",
    "      for i in range(10):\n",
    "          n_noisy = np.sum(np.array(self.targets) == i)\n",
    "          print(\"Noisy class %s, has %s samples.\" % (i, n_noisy))\n",
    "      return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T05:58:30.311017Z",
     "iopub.status.busy": "2025-03-29T05:58:30.310774Z",
     "iopub.status.idle": "2025-03-29T05:58:30.330982Z",
     "shell.execute_reply": "2025-03-29T05:58:30.330318Z",
     "shell.execute_reply.started": "2025-03-29T05:58:30.310987Z"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1742290020491,
     "user": {
      "displayName": "Soham Kalburgi",
      "userId": "01384065308802855965"
     },
     "user_tz": -330
    },
    "id": "1BU2RZtLlpOK",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DatasetGenerator():\n",
    "    def __init__(self,\n",
    "                 train_batch_size=128,\n",
    "                 eval_batch_size=128,\n",
    "                 data_path='data/',\n",
    "                 seed=123,\n",
    "                 num_of_workers=2,\n",
    "                 asym=True,\n",
    "                 noise_rate=0.2):\n",
    "        self.seed = seed\n",
    "        np.random.seed(seed)\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.eval_batch_size = eval_batch_size\n",
    "        self.data_path = data_path\n",
    "        self.num_of_workers = num_of_workers\n",
    "        self.noise_rate = noise_rate\n",
    "        self.asym = asym\n",
    "        self.data_loaders = self.loadData()\n",
    "        return\n",
    "\n",
    "    def getDataLoader(self):\n",
    "        return self.data_loaders\n",
    "\n",
    "    def loadData(self):\n",
    "            CIFAR_MEAN = [0.49139968, 0.48215827, 0.44653124]\n",
    "            CIFAR_STD = [0.24703233, 0.24348505, 0.26158768]\n",
    "\n",
    "            train_transform = transforms.Compose([\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(CIFAR_MEAN, CIFAR_STD)])\n",
    "\n",
    "            test_transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(CIFAR_MEAN, CIFAR_STD)])\n",
    "\n",
    "            train_dataset = Cifar10Noisy(root=self.data_path,\n",
    "                                         train=True,\n",
    "                                         transform=train_transform,\n",
    "                                         download=True,\n",
    "                                         asym=self.asym,\n",
    "                                         noise_rate=self.noise_rate)\n",
    "\n",
    "            test_dataset = datasets.CIFAR10(root=self.data_path,\n",
    "                                            train=False,\n",
    "                                            transform=test_transform,\n",
    "                                            download=True)\n",
    "            data_loaders = {}\n",
    "\n",
    "            data_loaders['train_dataset'] = DataLoader(dataset=train_dataset,\n",
    "                                                      batch_size=self.train_batch_size,\n",
    "                                                      shuffle=True,\n",
    "                                                      pin_memory=True,\n",
    "                                                      num_workers=self.num_of_workers)\n",
    "\n",
    "            data_loaders['test_dataset'] = DataLoader(dataset=test_dataset,\n",
    "                                                      batch_size=self.eval_batch_size,\n",
    "                                                      shuffle=False,\n",
    "                                                      pin_memory=True,\n",
    "                                                      num_workers=self.num_of_workers)\n",
    "\n",
    "            print(\"Num of train %d\" % (len(train_dataset)))\n",
    "            print(\"Num of test %d\" % (len(test_dataset)))\n",
    "\n",
    "            return data_loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEY6AaycrLG-"
   },
   "source": [
    "### Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T05:58:30.331911Z",
     "iopub.status.busy": "2025-03-29T05:58:30.331681Z",
     "iopub.status.idle": "2025-03-29T05:58:30.350235Z",
     "shell.execute_reply": "2025-03-29T05:58:30.349599Z",
     "shell.execute_reply.started": "2025-03-29T05:58:30.331893Z"
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1742292426331,
     "user": {
      "displayName": "Soham Kalburgi",
      "userId": "01384065308802855965"
     },
     "user_tz": -330
    },
    "id": "XL55HFMFrNSY",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FocalLoss(torch.nn.Module):\n",
    "    '''\n",
    "        https://github.com/clcarwin/focal_loss_pytorch/blob/master/focalloss.py\n",
    "    '''\n",
    "\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha, (float, int)):\n",
    "            self.alpha = torch.Tensor([alpha, 1-alpha])\n",
    "        if isinstance(alpha, list):\n",
    "            self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim() > 2:\n",
    "            input = input.view(input.size(0), input.size(1), -1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1, 2)                         # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1, input.size(2))    # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        logpt = logpt.gather(1, target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = torch.autograd.Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0, target.data.view(-1))\n",
    "            logpt = logpt * torch.autograd.Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T05:58:30.351248Z",
     "iopub.status.busy": "2025-03-29T05:58:30.350983Z",
     "iopub.status.idle": "2025-03-29T05:58:30.369327Z",
     "shell.execute_reply": "2025-03-29T05:58:30.368559Z",
     "shell.execute_reply.started": "2025-03-29T05:58:30.351221Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MeanAbsoluteError(torch.nn.Module):\n",
    "    def __init__(self, num_classes, scale=1.0):\n",
    "        super(MeanAbsoluteError, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        self.scale = scale\n",
    "        return\n",
    "\n",
    "    def forward(self, pred, labels):\n",
    "        pred = F.softmax(pred, dim=1)\n",
    "        label_one_hot = torch.nn.functional.one_hot(labels, self.num_classes).float().to(self.device)\n",
    "        mae = 1. - torch.sum(label_one_hot * pred, dim=1)\n",
    "        # Note: Reduced MAE\n",
    "        # Original: torch.abs(pred - label_one_hot).sum(dim=1)\n",
    "        # $MAE = \\sum_{k=1}^{K} |\\bm{p}(k|\\bm{x}) - \\bm{q}(k|\\bm{x})|$\n",
    "        # $MAE = \\sum_{k=1}^{K}\\bm{p}(k|\\bm{x}) - p(y|\\bm{x}) + (1 - p(y|\\bm{x}))$\n",
    "        # $MAE = 2 - 2p(y|\\bm{x})$\n",
    "        #\n",
    "        return self.scale * mae.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T05:58:30.371678Z",
     "iopub.status.busy": "2025-03-29T05:58:30.371463Z",
     "iopub.status.idle": "2025-03-29T05:58:30.385609Z",
     "shell.execute_reply": "2025-03-29T05:58:30.384991Z",
     "shell.execute_reply.started": "2025-03-29T05:58:30.371658Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ReverseCrossEntropy(torch.nn.Module):\n",
    "    def __init__(self, num_classes, scale=1.0):\n",
    "        super(ReverseCrossEntropy, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, pred, labels):\n",
    "        pred = F.softmax(pred, dim=1)\n",
    "        pred = torch.clamp(pred, min=1e-7, max=1.0)\n",
    "        label_one_hot = torch.nn.functional.one_hot(labels, self.num_classes).float().to(self.device)\n",
    "        label_one_hot = torch.clamp(label_one_hot, min=1e-4, max=1.0)\n",
    "        rce = (-1*torch.sum(pred * torch.log(label_one_hot), dim=1))\n",
    "        return self.scale * rce.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfb-HA2ms5Un"
   },
   "source": [
    "### Normalized Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T05:58:30.386900Z",
     "iopub.status.busy": "2025-03-29T05:58:30.386674Z",
     "iopub.status.idle": "2025-03-29T05:58:30.399861Z",
     "shell.execute_reply": "2025-03-29T05:58:30.399157Z",
     "shell.execute_reply.started": "2025-03-29T05:58:30.386882Z"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1742290020497,
     "user": {
      "displayName": "Soham Kalburgi",
      "userId": "01384065308802855965"
     },
     "user_tz": -330
    },
    "id": "Kp5VHtf8s4mS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NormalizedCrossEntropy(torch.nn.Module):\n",
    "    def __init__(self, num_classes, scale=1.0):\n",
    "        super(NormalizedCrossEntropy, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, pred, labels):\n",
    "        pred = F.log_softmax(pred, dim=1)\n",
    "        label_one_hot = torch.nn.functional.one_hot(labels, self.num_classes).float().to(self.device)\n",
    "        nce = -1 * torch.sum(label_one_hot * pred, dim=1) / (- pred.sum(dim=1))\n",
    "        return self.scale * nce.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T05:58:30.400776Z",
     "iopub.status.busy": "2025-03-29T05:58:30.400570Z",
     "iopub.status.idle": "2025-03-29T05:58:30.415710Z",
     "shell.execute_reply": "2025-03-29T05:58:30.415029Z",
     "shell.execute_reply.started": "2025-03-29T05:58:30.400758Z"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1742290020506,
     "user": {
      "displayName": "Soham Kalburgi",
      "userId": "01384065308802855965"
     },
     "user_tz": -330
    },
    "id": "qV83XPd_tOPl",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NormalizedFocalLoss(torch.nn.Module):\n",
    "    def __init__(self, scale=1.0, gamma=0, num_classes=10, alpha=None, size_average=True):\n",
    "        super(NormalizedFocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.size_average = size_average\n",
    "        self.num_classes = num_classes\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        target = target.view(-1, 1)\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        normalizor = torch.sum(-1 * (1 - logpt.data.exp()) ** self.gamma * logpt, dim=1)\n",
    "        logpt = logpt.gather(1, target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = torch.autograd.Variable(logpt.data.exp())\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        loss = self.scale * loss / normalizor\n",
    "\n",
    "        if self.size_average:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLFyob25SSr-"
   },
   "source": [
    "### APL Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T05:58:30.416600Z",
     "iopub.status.busy": "2025-03-29T05:58:30.416373Z",
     "iopub.status.idle": "2025-03-29T05:58:30.432925Z",
     "shell.execute_reply": "2025-03-29T05:58:30.432162Z",
     "shell.execute_reply.started": "2025-03-29T05:58:30.416580Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NFLandMAE(torch.nn.Module):\n",
    "    def __init__(self, alpha, beta, num_classes, gamma=0.5):\n",
    "        super(NFLandMAE, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.nfl = NormalizedFocalLoss(scale=alpha, gamma=gamma, num_classes=num_classes)\n",
    "        self.mae = MeanAbsoluteError(scale=beta, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, pred, labels):\n",
    "        return self.nfl(pred, labels) + self.mae(pred, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T05:58:30.433916Z",
     "iopub.status.busy": "2025-03-29T05:58:30.433649Z",
     "iopub.status.idle": "2025-03-29T05:58:30.446071Z",
     "shell.execute_reply": "2025-03-29T05:58:30.445396Z",
     "shell.execute_reply.started": "2025-03-29T05:58:30.433885Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NFLandRCE(torch.nn.Module):\n",
    "    def __init__(self, alpha, beta, num_classes, gamma=0.5):\n",
    "        super(NFLandRCE, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.nfl = NormalizedFocalLoss(scale=alpha, gamma=gamma, num_classes=num_classes)\n",
    "        self.rce = ReverseCrossEntropy(scale=beta, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, pred, labels):\n",
    "        return self.nfl(pred, labels) + self.rce(pred, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T05:58:30.447039Z",
     "iopub.status.busy": "2025-03-29T05:58:30.446790Z",
     "iopub.status.idle": "2025-03-29T05:58:30.459933Z",
     "shell.execute_reply": "2025-03-29T05:58:30.459200Z",
     "shell.execute_reply.started": "2025-03-29T05:58:30.447015Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NCEandMAE(torch.nn.Module):\n",
    "    def __init__(self, alpha, beta, num_classes):\n",
    "        super(NCEandMAE, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.nce = NormalizedCrossEntropy(scale=alpha, num_classes=num_classes)\n",
    "        self.mae = MeanAbsoluteError(scale=beta, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, pred, labels):\n",
    "        return self.nce(pred, labels) + self.mae(pred, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T05:58:30.461029Z",
     "iopub.status.busy": "2025-03-29T05:58:30.460763Z",
     "iopub.status.idle": "2025-03-29T05:58:30.476775Z",
     "shell.execute_reply": "2025-03-29T05:58:30.476192Z",
     "shell.execute_reply.started": "2025-03-29T05:58:30.461001Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NCEandRCE(torch.nn.Module):\n",
    "    def __init__(self, alpha, beta, num_classes):\n",
    "        super(NCEandRCE, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.nce = NormalizedCrossEntropy(scale=alpha, num_classes=num_classes)\n",
    "        self.rce = ReverseCrossEntropy(scale=beta, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, pred, labels):\n",
    "        return self.nce(pred, labels) + self.rce(pred, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itlQLI1QuzhD"
   },
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T05:58:30.477581Z",
     "iopub.status.busy": "2025-03-29T05:58:30.477394Z",
     "iopub.status.idle": "2025-03-29T05:58:30.494749Z",
     "shell.execute_reply": "2025-03-29T05:58:30.494108Z",
     "shell.execute_reply.started": "2025-03-29T05:58:30.477563Z"
    },
    "executionInfo": {
     "elapsed": 74,
     "status": "ok",
     "timestamp": 1742290020582,
     "user": {
      "displayName": "Soham Kalburgi",
      "userId": "01384065308802855965"
     },
     "user_tz": -330
    },
    "id": "xgSmQvZ8tnO1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "        self._reset_prams()\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "    def _reset_prams(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T05:58:30.495545Z",
     "iopub.status.busy": "2025-03-29T05:58:30.495366Z",
     "iopub.status.idle": "2025-03-29T05:58:30.508556Z",
     "shell.execute_reply": "2025-03-29T05:58:30.507895Z",
     "shell.execute_reply.started": "2025-03-29T05:58:30.495528Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ConvBrunch(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3):\n",
    "        super(ConvBrunch, self).__init__()\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        self.out_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(out_planes),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.out_conv(x)\n",
    "\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self, type='CIFAR10'):\n",
    "        super(ToyModel, self).__init__()\n",
    "        self.type = type\n",
    "        if type == 'CIFAR10':\n",
    "            self.block1 = nn.Sequential(\n",
    "                ConvBrunch(3, 64, 3),\n",
    "                ConvBrunch(64, 64, 3),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            self.block2 = nn.Sequential(\n",
    "                ConvBrunch(64, 128, 3),\n",
    "                ConvBrunch(128, 128, 3),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            self.block3 = nn.Sequential(\n",
    "                ConvBrunch(128, 196, 3),\n",
    "                ConvBrunch(196, 196, 3),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            # self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "            self.fc1 = nn.Sequential(\n",
    "                nn.Linear(4*4*196, 256),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.ReLU())\n",
    "            self.fc2 = nn.Linear(256, 10)\n",
    "            self.fc_size = 4*4*196\n",
    "        self._reset_prams()\n",
    "\n",
    "    def _reset_prams(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "        return\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x) if self.type == 'CIFAR10' else x\n",
    "        # x = self.global_avg_pool(x)\n",
    "        # x = x.view(x.shape[0], -1)\n",
    "        x = x.view(-1, self.fc_size)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnlLfLVju1fC"
   },
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T05:58:30.509592Z",
     "iopub.status.busy": "2025-03-29T05:58:30.509372Z",
     "iopub.status.idle": "2025-03-29T05:58:30.525646Z",
     "shell.execute_reply": "2025-03-29T05:58:30.524872Z",
     "shell.execute_reply.started": "2025-03-29T05:58:30.509572Z"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1742290020589,
     "user": {
      "displayName": "Soham Kalburgi",
      "userId": "01384065308802855965"
     },
     "user_tz": -330
    },
    "id": "aRAW58sAutSR",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, criterion, optimizer, scheduler=None):\n",
    "        self.model = model.to(device)\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.best_acc = 0\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(train_loader, desc='Training')):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            current_loss = total_loss / (batch_idx + 1)\n",
    "            current_acc = correct / total\n",
    "\n",
    "            wandb.log({\n",
    "                \"loss\": current_loss,\n",
    "                \"acc\": current_acc,\n",
    "                \"learning_rate\": self.scheduler.get_last_lr()[0] if self.scheduler else self.optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "\n",
    "        return current_loss, current_acc\n",
    "\n",
    "    def test(self, test_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in tqdm(test_loader, desc='Testing'):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        acc = correct / total\n",
    "        avg_loss = total_loss / len(test_loader)\n",
    "        wandb.log({\n",
    "            \"test_loss\": avg_loss,\n",
    "            \"test_acc\": acc\n",
    "        })\n",
    "\n",
    "        return avg_loss, acc\n",
    "\n",
    "    def train(self, train_loader, test_loader, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            print(f'\\nEpoch: {epoch+1}')\n",
    "\n",
    "            train_loss, train_acc = self.train_epoch(train_loader)\n",
    "            test_loss, test_acc = self.test(test_loader)\n",
    "\n",
    "            if self.scheduler:\n",
    "                self.scheduler.step()\n",
    "\n",
    "            if test_acc > self.best_acc:\n",
    "                self.best_acc = test_acc\n",
    "                wandb.run.summary[\"best_accuracy\"] = self.best_acc\n",
    "\n",
    "            print(f'Train Loss: {train_loss:.3f} | Train Acc: {train_acc:.2f}')\n",
    "            print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc:.2f}')\n",
    "            print(f'Best Acc: {self.best_acc:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2025-03-29T05:58:30.526735Z",
     "iopub.status.busy": "2025-03-29T05:58:30.526474Z"
    },
    "executionInfo": {
     "elapsed": 2281178,
     "status": "ok",
     "timestamp": 1742292301751,
     "user": {
      "displayName": "Soham Kalburgi",
      "userId": "01384065308802855965"
     },
     "user_tz": -330
    },
    "id": "fjyDty7r0Ei6",
    "outputId": "4edafb41-e1b8-450d-ec6e-96d3324aa0ac",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def main():\n",
    "    wandb.login(key='wandb.ai/authorize')\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"saidl\",\n",
    "        name = f\"nfl_run_{datetime.now().strftime('%d%m_%H%M')}\",\n",
    "        config={\n",
    "            \"epochs\": 50,\n",
    "            \"batch_size\": 128,\n",
    "            \"learning_rate\": 0.01,\n",
    "            \"momentum\": 0.9,\n",
    "            \"noise_rate\": 0.2,\n",
    "            \"weight_decay\": 1e-4,\n",
    "            \"model\": \"ResNet18\",\n",
    "            \"loss_function\": \"nfl\",\n",
    "            \"type\": \"asym\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    config = wandb.config\n",
    "\n",
    "    data_generator = DatasetGenerator(\n",
    "        train_batch_size=config.batch_size,\n",
    "        eval_batch_size=config.batch_size,\n",
    "        noise_rate=config.noise_rate,\n",
    "        data_path='./data'\n",
    "    )\n",
    "    data_loaders = data_generator.getDataLoader()\n",
    "    train_loader = data_loaders['train_dataset']\n",
    "    test_loader = data_loaders['test_dataset']\n",
    "\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2]) # ResNet18\n",
    "    # model = ToyModel() # 8-layer CNN\n",
    "    model.to(device)\n",
    "    wandb.watch(model, log=\"all\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # criterion = NormalizedCrossEntropy(num_classes=10)\n",
    "    # criterion = NormalizedFocalLoss(gamma=0.5)\n",
    "    # criterion = FocalLoss(gamma=0.5)\n",
    "    \n",
    "    # criterion = ReverseCrossEntropy(num_classes=10)\n",
    "    # criterion = MeanAbsoluteError(num_classes=10)\n",
    "\n",
    "    # APL Losses\n",
    "    # criterion = NFLandMAE(alpha=1.0, beta=1.0, num_classes=10, gamma=0.5)\n",
    "    # criterion = NFLandRCE(alpha=1.0, beta=1.0, num_classes=10, gamma=0.5)\n",
    "    # criterion = NCEandMAE(alpha=1.0, beta=1.0, num_classes=10)\n",
    "    # criterion = NCEandRCE(alpha=1.0, beta=1.0, num_classes=10)\n",
    "    \n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=config.learning_rate,\n",
    "        momentum=config.momentum,\n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs)\n",
    "\n",
    "    trainer = Trainer(model, criterion, optimizer, scheduler=scheduler)\n",
    "    trainer.train(train_loader, test_loader, config.epochs)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
