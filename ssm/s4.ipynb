{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNtKSanXUhINnwNGLDuvjz2"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### S4","metadata":{"id":"QYpyqK41rLG-"}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch import view_as_real\nfrom torch.fft import ifft, irfft, rfft\nfrom torch.nn import functional as F\n\n\ndef log_step_init(tensor, dt_min=0.001, dt_max=0.1):\n    scale = torch.log(torch.tensor(dt_max)) - torch.log(torch.tensor(dt_min))\n    return tensor * scale + torch.log(torch.tensor(dt_min))\n\ndef hippo(N):\n    P = torch.sqrt(1 + 2 * torch.arange(1, N+1, dtype=torch.float))\n    A = torch.outer(P, P)\n    A = torch.tril(A) - torch.diag(torch.arange(1, N+1, dtype=torch.float))\n    return A\n\ndef hippo_dplr(N):\n    A = -1 * hippo(N)  # -ve sign here\n\n    p = 0.5 * torch.sqrt(2 * torch.arange(1, N+1, dtype=torch.float32) + 1.0)\n    p = p.to(torch.complex64)\n    \n    Ap = A.to(torch.complex64) + torch.outer(p, p)\n    \n    # eigen values, vectors\n    lambda_, V = torch.linalg.eig(Ap)\n\n    return lambda_, p, V\n\n\ndef p_lambda(n):\n    lambda_, p, V = hippo_dplr(n)\n    Vc = V.conj().T\n    p = Vc @ p\n    return [p, lambda_]\n\n\ndef cauchy_kernel(v, omega, lambda_):\n    if v.ndim == 1:\n        v = v.unsqueeze(0).unsqueeze(0)\n    elif v.ndim == 2:\n        v = v.unsqueeze(1)\n    return (v/(omega-lambda_)).sum(dim=-1)\n\n\ndef causal_convolution(u, K):\n    l_max = u.shape[1]  # u.shape = [batch, seq_length, d_model]\n    \n    # pad seq_length with l_max zeroes and compute fft\n    ud = rfft(F.pad(u.float(), pad=(0, 0, 0, l_max, 0, 0)), dim=1)\n    Kd = rfft(F.pad(K.float(), pad=(0, l_max)), dim=-1)\n    \n    # freq -> time domain\n    return irfft(ud.transpose(-2, -1)*Kd)[..., :l_max].transpose(-2, -1).type_as(u)\n\n# compute frequencies\ndef f_omega(l_max, dtype=torch.complex64):\n    return torch.arange(l_max).type(dtype).mul(2j * torch.tensor(torch.pi) / l_max).exp()\n\n\nclass S4Layer(nn.Module):\n    def __init__(self, d_model, n, l_max, time_delta=0.1):\n        super().__init__()\n        self.d_model = d_model\n        self.n = n\n        self.l_max = l_max\n        self.time_delta = time_delta\n        \n        p, lambda_ = p_lambda(n)\n        p = p.to(torch.complex64)\n        lambda_ = lambda_.to(torch.complex64)\n        self._p = nn.Parameter(view_as_real(p))\n        self._lambda_ = nn.Parameter(view_as_real(lambda_).unsqueeze(0).unsqueeze(1))\n        \n        # make non trainable\n        self.register_buffer(\n            \"omega\",\n            tensor=f_omega(self.l_max, dtype=torch.complex64),\n        )\n        self.register_buffer(\n            \"ifft_order\",\n            tensor=torch.tensor(\n                [i if i == 0 else self.l_max-i for i in range(self.l_max)],\n                dtype=torch.long,\n            ),\n        )\n        \n        B_init = torch.sqrt(2 * torch.arange(1, n+1, dtype=torch.float32) + 1.0)\n        B_init = B_init.repeat(d_model, 1)\n        self._B = nn.Parameter(\n            view_as_real(B_init.to(torch.complex64))\n        )\n        self._Ct = nn.Parameter(\n            view_as_real(\n                nn.init.xavier_normal_(torch.empty(d_model, n, dtype=torch.complex64))\n            )\n        )\n        self.D = nn.Parameter(torch.ones(1, 1, d_model))\n        self.log_step = nn.Parameter(log_step_init(torch.rand(d_model)))\n\n    @property\n    def p(self):\n        return torch.view_as_complex(self._p)\n\n    @property\n    def lambda_(self):\n        return torch.view_as_complex(self._lambda_)\n\n    @property\n    def B(self):\n        return torch.view_as_complex(self._B)\n\n    @property\n    def Ct(self):\n        return torch.view_as_complex(self._Ct)\n\n    def roots(self):\n        a0 = self.Ct.conj()\n        a1 = self.p.conj().unsqueeze(0)\n        b0 = self.B\n        b1 = self.p.unsqueeze(0)\n        step = self.log_step.exp()     \n\n        # asynchronous discretization\n        lambda_d = torch.exp(self.lambda_ * step.unsqueeze(1).unsqueeze(2) * self.time_delta)\n        delta = (torch.exp(self.lambda_ * step.unsqueeze(1).unsqueeze(2)) - 1) / self.lambda_\n        omega_z = self.omega.unsqueeze(0).unsqueeze(2)\n\n        k00 = cauchy_kernel((a0 * b0).unsqueeze(1) * delta, omega_z, lambda_d)\n        k01 = cauchy_kernel((a0 * b1).unsqueeze(1) * delta, omega_z, lambda_d)\n        k10 = cauchy_kernel((a1 * b0).unsqueeze(1) * delta, omega_z, lambda_d)\n        k11 = cauchy_kernel((a1 * b1).unsqueeze(1) * delta, omega_z, lambda_d)\n        \n        return k00 - k01 * (1.0 / (1.0 + k11)) * k10\n\n    @property\n    def K(self):\n        at_roots = self.roots()\n        out = ifft(at_roots, n=self.l_max, dim=-1)\n        conv = torch.stack([i[self.ifft_order] for i in out]).real\n        return conv.unsqueeze(0)\n\n    def forward(self, u):\n        return causal_convolution(u, K=self.K) + (self.D * u)\n\n\nclass S4Block(nn.Module):\n    def __init__(\n        self,\n        d_model,\n        n,\n        l_max,\n        dropout=0.0,\n    ):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(d_model)\n        self.s4 = S4Layer(d_model, n=n, l_max=l_max)\n        self.activation = nn.GELU()\n        self.dropout = nn.Dropout(dropout)\n        self.linear = nn.Linear(d_model, d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n    \n    def forward(self, x):\n        residual = x\n        x = self.norm1(x)\n        x = self.s4(x)\n        x = self.activation(x)\n        x = self.dropout(x)\n        x = self.linear(x)\n        x = x + residual\n\n        return x\n\n\nclass S4Model(nn.Module):\n    def __init__(\n        self,\n        d_input,\n        d_model,\n        d_output,\n        n_blocks,\n        n,\n        l_max,\n        dropout = 0.0,\n    ):\n        super().__init__()\n        self.d_input = d_input\n        self.d_model = d_model\n        self.d_output = d_output\n        self.n_blocks = n_blocks\n        \n        self.encoder = nn.Linear(d_input, d_model)\n        self.blocks = nn.ModuleList([\n            S4Block(\n                d_model=d_model,\n                n=n,\n                l_max=l_max,\n                dropout=dropout,\n            ) for _ in range(n_blocks)\n        ])\n        self.decoder = nn.Linear(d_model, d_output)\n\n    def forward(self, u):\n        x = self.encoder(u)\n        for block in self.blocks:\n            x = block(x)\n        x = x.mean(dim=1)\n        x = self.decoder(x)\n        return x\n    ","metadata":{"id":"dSRVmZJKFyUD","executionInfo":{"status":"ok","timestamp":1742906046930,"user_tz":-330,"elapsed":9604,"user":{"displayName":"Soham Kalburgi","userId":"01384065308802855965"}},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Main","metadata":{"id":"XDsizABErNm8"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom tqdm import tqdm\nimport wandb\nfrom datetime import datetime\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nwandb.login(key=\"2f3ffd7baf545af396e18e48bfa20b33d2609dcc\")\n\ndef split_train_val(train, val_split):\n    train_len = int(len(train) * (1.0 - val_split))\n    train, val = torch.utils.data.random_split(\n        train,\n        (train_len, len(train) - train_len),\n        generator=torch.Generator().manual_seed(42),\n    )\n    return train, val\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    transforms.Lambda(lambda x: x.reshape(3, -1).t())\n])\n\ntrainset = torchvision.datasets.CIFAR10(\n    root='./data/cifar/', train=True, download=True, transform=transform)\ntrainset, _ = split_train_val(trainset, val_split=0.1)\n\nvalset = torchvision.datasets.CIFAR10(\n    root='./data/cifar/', train=True, download=True, transform=transform)\n_, valset = split_train_val(valset, val_split=0.1)\n\ntestset = torchvision.datasets.CIFAR10(\n    root='./data/cifar/', train=False, download=True, transform=transform)\n\nd_input = 3\nd_output = 10\n\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\nvalloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=False)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n\nmodel = S4Model(\n    d_input=d_input,\n    d_model=512,\n    d_output=d_output,\n    n_blocks=6,\n    n=64,\n    l_max=1024,\n    dropout=0.2,\n)\n\nmodel.to(device)\n\nnum_epochs = 8\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n\n\nwandb.init(\n    project=\"saidl-s4\",\n    name = f\"s4_run_{datetime.now().strftime('%d%m_%H%M')}\",\n    config={\n        \"learning_rate\": 0.001,\n        \"weight_decay\": 0.01,\n        \"epochs\": num_epochs,\n        \"batch_size\": 64,\n        \"model_config\": {\n            \"d_model\": 128,\n            \"n_blocks\": 6,\n            \"dropout\": 0.2\n        },\n        \"discretization\": \"async\"\n    }\n)\n\ndef train_epoch(model, trainloader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    pbar = tqdm(trainloader, desc='training')\n    for i, (inputs, labels) in enumerate(pbar):\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n\n        optimizer.step()\n\n        running_loss = (running_loss * i + loss.item()) / (i + 1)\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n        current_acc = correct / total\n\n        wandb.log({\n            \"train/batch_loss\": loss.item(),\n            \"train/running_loss\": running_loss,\n            \"train/running_acc\": current_acc\n        })\n\n        pbar.set_postfix({\n            'loss': f'{running_loss:.4f}',\n            'acc': f'{correct/total:.2f}'\n        })\n\n    return running_loss, correct / total\n\ndef evaluate(model, dataloader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        pbar = tqdm(dataloader, desc='evaluating')\n        for i, (inputs, labels) in enumerate(pbar):\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            running_loss = (running_loss * i + loss.item()) / (i + 1)\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n\n            pbar.set_postfix({\n                'loss': f'{running_loss:.4f}',\n                'acc': f'{correct/total:.2f}'\n            })\n\n    return running_loss, correct / total\n\nbest_val_acc = 0.0\nfor epoch in range(num_epochs):\n    train_loss, train_acc = train_epoch(model, trainloader, criterion, optimizer, device)\n    val_loss, val_acc = evaluate(model, valloader, criterion, device)\n    scheduler.step()\n\n    wandb.log({\n        \"train/epoch_loss\": train_loss,\n        \"train/epoch_acc\": train_acc,\n        \"val/loss\": val_loss,\n        \"val/acc\": val_acc,\n        \"epoch\": epoch\n    })\n\n    print(f'epoch: {epoch+1}/{num_epochs}')\n    print(f'train loss: {train_loss:.4f} | train acc: {train_acc:.2f}')\n    print(f'val loss: {val_loss:.4f} | val acc: {val_acc:.2f}')\n\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), 'best_model.pth')\n\n    print('-' * 50)\n\nmodel.load_state_dict(torch.load('best_model.pth'))\ntest_loss, test_acc = evaluate(model, testloader, criterion, device)\nwandb.log({\n    \"test/loss\": test_loss,\n    \"test/acc\": test_acc\n})\nprint(f'test loss: {test_loss:.4f} | test acc: {test_acc:.2f}')\n\nwandb.finish()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"V-zhzlguqlLf","executionInfo":{"status":"error","timestamp":1742907223174,"user_tz":-330,"elapsed":1176239,"user":{"displayName":"Soham Kalburgi","userId":"01384065308802855965"}},"outputId":"9c4af410-751c-4677-96a3-b8debb2fea1d","trusted":true},"outputs":[],"execution_count":null}]}