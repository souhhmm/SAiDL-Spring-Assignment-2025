{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYpyqK41rLG-"
   },
   "source": [
    "### S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9604,
     "status": "ok",
     "timestamp": 1742906046930,
     "user": {
      "displayName": "Soham Kalburgi",
      "userId": "01384065308802855965"
     },
     "user_tz": -330
    },
    "id": "dSRVmZJKFyUD",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import view_as_real\n",
    "from torch.fft import ifft, irfft, rfft\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def log_step_init(tensor, dt_min=0.001, dt_max=0.1):\n",
    "    scale = torch.log(torch.tensor(dt_max)) - torch.log(torch.tensor(dt_min))\n",
    "    return tensor * scale + torch.log(torch.tensor(dt_min))\n",
    "\n",
    "def hippo(N):\n",
    "    P = torch.sqrt(1 + 2 * torch.arange(1, N+1, dtype=torch.float))\n",
    "    A = torch.outer(P, P)\n",
    "    A = torch.tril(A) - torch.diag(torch.arange(1, N+1, dtype=torch.float))\n",
    "    return A\n",
    "\n",
    "def hippo_dplr(N):\n",
    "    A = -1 * hippo(N)  # -ve sign here\n",
    "\n",
    "    p = 0.5 * torch.sqrt(2 * torch.arange(1, N+1, dtype=torch.float32) + 1.0)\n",
    "    p = p.to(torch.complex64)\n",
    "    \n",
    "    Ap = A.to(torch.complex64) + torch.outer(p, p)\n",
    "    \n",
    "    # eigen values, vectors\n",
    "    lambda_, V = torch.linalg.eig(Ap)\n",
    "\n",
    "    return lambda_, p, V\n",
    "\n",
    "\n",
    "def p_lambda(n):\n",
    "    lambda_, p, V = hippo_dplr(n)\n",
    "    Vc = V.conj().T\n",
    "    p = Vc @ p\n",
    "    return [p, lambda_]\n",
    "\n",
    "\n",
    "def cauchy_kernel(v, omega, lambda_):\n",
    "    if v.ndim == 1:\n",
    "        v = v.unsqueeze(0).unsqueeze(0)\n",
    "    elif v.ndim == 2:\n",
    "        v = v.unsqueeze(1)\n",
    "    return (v/(omega-lambda_)).sum(dim=-1)\n",
    "\n",
    "\n",
    "def causal_convolution(u, K):\n",
    "    l_max = u.shape[1]  # u.shape = [batch, seq_length, d_model]\n",
    "    \n",
    "    # pad seq_length with l_max zeroes and compute fft\n",
    "    ud = rfft(F.pad(u.float(), pad=(0, 0, 0, l_max, 0, 0)), dim=1)\n",
    "    Kd = rfft(F.pad(K.float(), pad=(0, l_max)), dim=-1)\n",
    "    \n",
    "    # freq -> time domain\n",
    "    return irfft(ud.transpose(-2, -1)*Kd)[..., :l_max].transpose(-2, -1).type_as(u)\n",
    "\n",
    "# compute frequencies\n",
    "def f_omega(l_max, dtype=torch.complex64):\n",
    "    return torch.arange(l_max).type(dtype).mul(2j * torch.tensor(torch.pi) / l_max).exp()\n",
    "\n",
    "\n",
    "class S4Layer(nn.Module):\n",
    "    def __init__(self, d_model, n, l_max, time_delta=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n = n\n",
    "        self.l_max = l_max\n",
    "        self.time_delta = time_delta\n",
    "        \n",
    "        p, lambda_ = p_lambda(n)\n",
    "        p = p.to(torch.complex64)\n",
    "        lambda_ = lambda_.to(torch.complex64)\n",
    "        self._p = nn.Parameter(view_as_real(p))\n",
    "        self._lambda_ = nn.Parameter(view_as_real(lambda_).unsqueeze(0).unsqueeze(1))\n",
    "        \n",
    "        # make non trainable\n",
    "        self.register_buffer(\n",
    "            \"omega\",\n",
    "            tensor=f_omega(self.l_max, dtype=torch.complex64),\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"ifft_order\",\n",
    "            tensor=torch.tensor(\n",
    "                [i if i == 0 else self.l_max-i for i in range(self.l_max)],\n",
    "                dtype=torch.long,\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        B_init = torch.sqrt(2 * torch.arange(1, n+1, dtype=torch.float32) + 1.0)\n",
    "        B_init = B_init.repeat(d_model, 1)\n",
    "        self._B = nn.Parameter(\n",
    "            view_as_real(B_init.to(torch.complex64))\n",
    "        )\n",
    "        self._Ct = nn.Parameter(\n",
    "            view_as_real(\n",
    "                nn.init.xavier_normal_(torch.empty(d_model, n, dtype=torch.complex64))\n",
    "            )\n",
    "        )\n",
    "        self.D = nn.Parameter(torch.ones(1, 1, d_model))\n",
    "        self.log_step = nn.Parameter(log_step_init(torch.rand(d_model)))\n",
    "\n",
    "    @property\n",
    "    def p(self):\n",
    "        return torch.view_as_complex(self._p)\n",
    "\n",
    "    @property\n",
    "    def lambda_(self):\n",
    "        return torch.view_as_complex(self._lambda_)\n",
    "\n",
    "    @property\n",
    "    def B(self):\n",
    "        return torch.view_as_complex(self._B)\n",
    "\n",
    "    @property\n",
    "    def Ct(self):\n",
    "        return torch.view_as_complex(self._Ct)\n",
    "\n",
    "    def roots(self):\n",
    "        a0 = self.Ct.conj()\n",
    "        a1 = self.p.conj().unsqueeze(0)\n",
    "        b0 = self.B\n",
    "        b1 = self.p.unsqueeze(0)\n",
    "        step = self.log_step.exp()     \n",
    "\n",
    "        # asynchronous discretization\n",
    "        lambda_d = torch.exp(self.lambda_ * step.unsqueeze(1).unsqueeze(2) * self.time_delta)\n",
    "        delta = (torch.exp(self.lambda_ * step.unsqueeze(1).unsqueeze(2)) - 1) / self.lambda_\n",
    "        omega_z = self.omega.unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "        k00 = cauchy_kernel((a0 * b0).unsqueeze(1) * delta, omega_z, lambda_d)\n",
    "        k01 = cauchy_kernel((a0 * b1).unsqueeze(1) * delta, omega_z, lambda_d)\n",
    "        k10 = cauchy_kernel((a1 * b0).unsqueeze(1) * delta, omega_z, lambda_d)\n",
    "        k11 = cauchy_kernel((a1 * b1).unsqueeze(1) * delta, omega_z, lambda_d)\n",
    "        \n",
    "        return k00 - k01 * (1.0 / (1.0 + k11)) * k10\n",
    "\n",
    "    @property\n",
    "    def K(self):\n",
    "        at_roots = self.roots()\n",
    "        out = ifft(at_roots, n=self.l_max, dim=-1)\n",
    "        conv = torch.stack([i[self.ifft_order] for i in out]).real\n",
    "        return conv.unsqueeze(0)\n",
    "\n",
    "    def forward(self, u):\n",
    "        return causal_convolution(u, K=self.K) + (self.D * u)\n",
    "\n",
    "\n",
    "class S4Block(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        n,\n",
    "        l_max,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.s4 = S4Layer(d_model, n=n, l_max=l_max)\n",
    "        self.activation = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.s4(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        x = x + residual\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class S4Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_input,\n",
    "        d_model,\n",
    "        d_output,\n",
    "        n_blocks,\n",
    "        n,\n",
    "        l_max,\n",
    "        dropout = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_input = d_input\n",
    "        self.d_model = d_model\n",
    "        self.d_output = d_output\n",
    "        self.n_blocks = n_blocks\n",
    "        \n",
    "        self.encoder = nn.Linear(d_input, d_model)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            S4Block(\n",
    "                d_model=d_model,\n",
    "                n=n,\n",
    "                l_max=l_max,\n",
    "                dropout=dropout,\n",
    "            ) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.decoder = nn.Linear(d_model, d_output)\n",
    "\n",
    "    def forward(self, u):\n",
    "        x = self.encoder(u)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDsizABErNm8"
   },
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1176239,
     "status": "error",
     "timestamp": 1742907223174,
     "user": {
      "displayName": "Soham Kalburgi",
      "userId": "01384065308802855965"
     },
     "user_tz": -330
    },
    "id": "V-zhzlguqlLf",
    "outputId": "9c4af410-751c-4677-96a3-b8debb2fea1d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "wandb.login(key=\"wandb.ai/authorize\")\n",
    "\n",
    "def split_train_val(train, val_split):\n",
    "    train_len = int(len(train) * (1.0 - val_split))\n",
    "    train, val = torch.utils.data.random_split(\n",
    "        train,\n",
    "        (train_len, len(train) - train_len),\n",
    "        generator=torch.Generator().manual_seed(42),\n",
    "    )\n",
    "    return train, val\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    transforms.Lambda(lambda x: x.reshape(3, -1).t())\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data/cifar/', train=True, download=True, transform=transform)\n",
    "trainset, _ = split_train_val(trainset, val_split=0.1)\n",
    "\n",
    "valset = torchvision.datasets.CIFAR10(\n",
    "    root='./data/cifar/', train=True, download=True, transform=transform)\n",
    "_, valset = split_train_val(valset, val_split=0.1)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data/cifar/', train=False, download=True, transform=transform)\n",
    "\n",
    "d_input = 3\n",
    "d_output = 10\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "model = S4Model(\n",
    "    d_input=d_input,\n",
    "    d_model=512,\n",
    "    d_output=d_output,\n",
    "    n_blocks=6,\n",
    "    n=64,\n",
    "    l_max=1024,\n",
    "    dropout=0.2,\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 8\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    project=\"saidl-s4\",\n",
    "    name = f\"s4_run_{datetime.now().strftime('%d%m_%H%M')}\",\n",
    "    config={\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"epochs\": num_epochs,\n",
    "        \"batch_size\": 64,\n",
    "        \"model_config\": {\n",
    "            \"d_model\": 128,\n",
    "            \"n_blocks\": 6,\n",
    "            \"dropout\": 0.2\n",
    "        },\n",
    "        \"discretization\": \"async\"\n",
    "    }\n",
    ")\n",
    "\n",
    "def train_epoch(model, trainloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(trainloader, desc='training')\n",
    "    for i, (inputs, labels) in enumerate(pbar):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss = (running_loss * i + loss.item()) / (i + 1)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        current_acc = correct / total\n",
    "\n",
    "        wandb.log({\n",
    "            \"train/batch_loss\": loss.item(),\n",
    "            \"train/running_loss\": running_loss,\n",
    "            \"train/running_acc\": current_acc\n",
    "        })\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{running_loss:.4f}',\n",
    "            'acc': f'{correct/total:.2f}'\n",
    "        })\n",
    "\n",
    "    return running_loss, correct / total\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc='evaluating')\n",
    "        for i, (inputs, labels) in enumerate(pbar):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss = (running_loss * i + loss.item()) / (i + 1)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{running_loss:.4f}',\n",
    "                'acc': f'{correct/total:.2f}'\n",
    "            })\n",
    "\n",
    "    return running_loss, correct / total\n",
    "\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, trainloader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = evaluate(model, valloader, criterion, device)\n",
    "    scheduler.step()\n",
    "\n",
    "    wandb.log({\n",
    "        \"train/epoch_loss\": train_loss,\n",
    "        \"train/epoch_acc\": train_acc,\n",
    "        \"val/loss\": val_loss,\n",
    "        \"val/acc\": val_acc,\n",
    "        \"epoch\": epoch\n",
    "    })\n",
    "\n",
    "    print(f'epoch: {epoch+1}/{num_epochs}')\n",
    "    print(f'train loss: {train_loss:.4f} | train acc: {train_acc:.2f}')\n",
    "    print(f'val loss: {val_loss:.4f} | val acc: {val_acc:.2f}')\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    print('-' * 50)\n",
    "\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "test_loss, test_acc = evaluate(model, testloader, criterion, device)\n",
    "wandb.log({\n",
    "    \"test/loss\": test_loss,\n",
    "    \"test/acc\": test_acc\n",
    "})\n",
    "print(f'test loss: {test_loss:.4f} | test acc: {test_acc:.2f}')\n",
    "\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNtKSanXUhINnwNGLDuvjz2",
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
